---
phase: 12-module-3-content
plan: 04
type: execute
wave: 4
depends_on: ["12-03"]
files_modified:
  - src/lib/content/module3/sentiment-analysis.ts
  - src/lib/content/module3/python-sentiment-demo.ts
  - src/lib/content/module3/index.ts
  - src/lib/data/modules.ts
  - src/lib/glossary/types.ts
  - src/lib/glossary/concepts/module3.ts
  - src/lib/glossary/index.ts
  - src/lib/exam/questions/module3.ts
  - src/lib/exam/index.ts
autonomous: false
---

<objective>
Create Lectures 9-10: Sentiment Analysis Application and Python Sentiment Demo, plus Module 3 glossary and exam questions.

Purpose: Complete Module 3 with sentiment analysis theory (BERT/DistilBERT) and practical Python demo (NLP pipeline), plus supporting exam prep content.

Output: Complete Module 3 with all 10 lectures, ~40 glossary terms, and ~50 exam questions ready.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-module-3-content/12-CONTEXT.md
@.planning/phases/12-module-3-content/12-03-SUMMARY.md

Source patterns:
@src/lib/content/module3/index.ts
@src/lib/glossary/types.ts
@src/lib/glossary/concepts/module1.ts
@src/lib/glossary/concepts/module2.ts
@src/lib/glossary/index.ts
@src/lib/exam/questions/module2.ts
@src/lib/exam/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Lecture 9 content - Sentiment Analysis Application</name>
  <files>src/lib/content/module3/sentiment-analysis.ts</files>
  <action>
Create content for sentiment analysis theory and BERT comparison.

Sections:

1. **Sentiment Analysis Purpose** (type: 'text')
   - Automatic classification of text by emotional tone
   - Output: positive / negative / neutral
   - Use case: Amazon reviews, consumer electronics

2. **Dictionary-Based Methods** (type: 'explanation')
   Create animated comparison:
   - Step 1: "VADER" - words have pre-assigned scores
   - Step 2: How it works: sum up word scores
   - Step 3: Problem: "This phone is sick" → Dictionary says negative
   - Step 4: Reality: Context makes it positive (slang)
   - Step 5: Limitation: no context understanding, sarcasm fails

3. **BERT Introduction** (type: 'explanation')
   Animated BERT explanation:
   - Step 1: "BERT" - Bidirectional Encoder Representations from Transformers
   - Step 2: "Bidirectional" - considers left AND right context
   - Step 3: Contrast with old models (left-to-right only)
   - Step 4: Result: understands "sick" means positive in context

4. **Pretraining Tasks** (type: 'diagram')
   Show BERT training:
   - Node: "Masked Language Modeling" - guess hidden words
   - Node: "Next Sentence Prediction" - is sentence B a follow-up?
   - Combined: learns language patterns at scale

5. **Fine-Tuning** (type: 'text')
   - Pre-trained model adapted to specific task
   - Sentiment classification: positive/negative
   - Output includes confidence score (0-1)
   - DistilBERT: faster, lighter, nearly same quality

6. **BERT vs Dictionary** (type: 'diagram')
   Show comparison:
   - "Dictionary Methods": fast, simple, no context
   - "BERT/Transformer": slower, complex, full context
   - Use case guidance: scale vs accuracy trade-off
  </action>
  <verify>TypeScript compiles, exports lectureContent object</verify>
  <done>Lecture 9 complete with BERT vs dictionary comparison and bidirectional context explanation</done>
</task>

<task type="auto">
  <name>Task 2: Create Lecture 10 content - Python Sentiment Demo</name>
  <files>src/lib/content/module3/python-sentiment-demo.ts</files>
  <action>
Create content for practical NLP pipeline implementation.

Sections:

1. **NLP Pipeline Overview** (type: 'text')
   - Full flow: raw text → sentiment classification
   - Libraries: pandas, spaCy, transformers (Hugging Face)
   - Data source: Amazon reviews dataset

2. **Text Preprocessing Steps** (type: 'explanation')
   Create animated pipeline:
   - Step 1: "Tokenization" - split text into words/tokens
   - Step 2: "Lemmatization" - reduce to base form (running → run)
   - Step 3: "POS Tagging" - identify parts of speech
   - Step 4: "Stop Word Removal" - remove 'the', 'is', 'and'
   - Step 5: "Clean Text" - ready for analysis

3. **spaCy Processing** (type: 'diagram')
   Show spaCy flow:
   - Input: "I love this device"
   - Tokens: [I] [love] [this] [device]
   - Lemmas: [I] [love] [this] [device]
   - POS: [PRON] [VERB] [DET] [NOUN]
   - Dependencies: tree structure

4. **Transformer Pipeline** (type: 'text')
   - Hugging Face transformers library
   - Pre-built sentiment pipeline
   - Input: text → Output: {label, score}
   - Example: "We are happy" → Positive, 0.9998

5. **Batch Processing** (type: 'diagram')
   Show mass classification:
   - "30 Reviews" → "Sentiment Model" → "Labels + Scores"
   - Row-wise inference with progress tracking
   - Result: DataFrame with sentiment column

6. **Business Applications** (type: 'text')
   - Aggregate customer sentiment
   - Track sentiment over time
   - Identify product problems
   - Spot trends early
   - Model: Text → Sentiment → Aggregation → Insight
   - Limitation: even transformers imperfect (domain, irony)
  </action>
  <verify>TypeScript compiles, exports lectureContent object</verify>
  <done>Lecture 10 complete with NLP pipeline visualization and preprocessing steps</done>
</task>

<task type="auto">
  <name>Task 3: Add Module 3 glossary terms</name>
  <files>src/lib/glossary/types.ts, src/lib/glossary/concepts/module3.ts, src/lib/glossary/index.ts</files>
  <action>
First, update src/lib/glossary/types.ts to add new category 'nlp' for NLP concepts:

```typescript
export type ConceptCategory =
  | 'foundation'
  | 'method'
  | 'assumption'
  | 'estimator'
  | 'bias'
  | 'design'
  | 'dag'
  | 'nlp';  // Add this

// Update categoryInfo:
export const categoryInfo: Record<ConceptCategory, ConceptCategoryInfo> = {
  // ... existing entries ...
  nlp: { id: 'nlp', label: 'NLP & Text', color: 'violet' }
};

// Update allCategories array
export const allCategories: ConceptCategory[] = [
  // ... existing ...
  'nlp'
];
```

Create src/lib/glossary/concepts/module3.ts with ~40 concepts following module1.ts pattern:

**Content Types (category: 'foundation'):**
- User-Generated Content (UGC)
- Firm-Generated Content (FGC)
- AI-Generated Content (AGC)
- Content Authenticity
- Content Receptivity
- Social Listening

**UGC Metrics (category: 'method'):**
- Net Promoter Score (NPS)
- Ratings (as metric)
- Chatter
- Valence
- Negativity Bias
- Fit Uncertainty
- Social Currency
- Emotional Resonance

**FGC Concepts (category: 'method'):**
- Managerial Response
- Response Personalization
- Customer Susceptibility

**AGC/LLM (category: 'nlp'):**
- Large Language Models
- Transformer Architecture
- Attention Mechanism
- Perceived Authenticity
- Content Homogenization
- Prompt Engineering

**Risks (category: 'bias'):**
- Fake Reviews
- Misinformation
- Echo Chambers
- Moderation Bias
- Willingness to Pay

**Influencer (category: 'method'):**
- Influencer
- Engagement Rate
- Brand-Influencer Fit
- Live Stream Commerce
- Inverted U Relationship

**NLP (category: 'nlp'):**
- Sentiment Analysis
- BERT
- DistilBERT
- Tokenization
- Lemmatization
- Stop Words
- Part-of-Speech Tagging

Update src/lib/glossary/index.ts to import and export module3Concepts:

```typescript
export { module3Concepts } from './concepts/module3';

import { module3Concepts } from './concepts/module3';
export const allConcepts = [...module1Concepts, ...module2Concepts, ...module3Concepts];
```

Each term needs: id, term, definition, category, relatedConcepts, lectureId, moduleId: '3', tags.
  </action>
  <verify>TypeScript compiles, glossary exports all new terms, filter by Module 3 works</verify>
  <done>~40 Module 3 glossary terms added with new 'nlp' category</done>
</task>

<task type="auto">
  <name>Task 4: Add Module 3 exam questions</name>
  <files>src/lib/exam/questions/module3.ts, src/lib/exam/index.ts</files>
  <action>
Create src/lib/exam/questions/module3.ts with 50 questions following module2.ts pattern:

Distribution by lecture:
- L1 (Intro Online Content): 5 questions (1 easy, 3 medium, 1 hard)
- L2 (Text History): 4 questions (1 easy, 2 medium, 1 hard)
- L3 (Types of Content): 5 questions (1 easy, 3 medium, 1 hard)
- L4 (UGC Deep Dive): 6 questions (1 easy, 3 medium, 2 hard)
- L5 (FGC Deep Dive): 6 questions (1 easy, 3 medium, 2 hard)
- L6 (AGC Deep Dive): 6 questions (1 easy, 3 medium, 2 hard)
- L7 (Emerging Concerns): 5 questions (1 easy, 3 medium, 1 hard)
- L8 (Influencer Marketing): 5 questions (1 easy, 3 medium, 1 hard)
- L9 (Sentiment Analysis): 4 questions (1 easy, 2 medium, 1 hard)
- L10 (Python Demo): 4 questions (1 easy, 2 medium, 1 hard)

Total: 50 questions (10 easy, 27 medium, 13 hard)

Question types:
- Conceptual: "What is the difference between UGC and FGC?"
- Application: "When would a brand use rational vs emotional responses?"
- Trade-off analysis: "What are the tradeoffs of AI-generated content?"
- Scenario: "A company sees high chatter but low valence. What does this indicate?"
- Technical: "Why is bidirectional context important for BERT?"

Each question: { id: 'm3-l{N}-{difficulty}{num}', question, answer, difficulty, lectureId }

Update src/lib/exam/index.ts:

```typescript
export { module3Questions } from './questions/module3';

import { module3Questions } from './questions/module3';
export const allQuestions = [...module1Questions, ...module2Questions, ...module3Questions];

// Update getQuestionsByModule function to include module3
```
  </action>
  <verify>TypeScript compiles, questions array includes Module 3</verify>
  <done>50 Module 3 exam questions added</done>
</task>

<task type="auto">
  <name>Task 5: Finalize module3 exports and mark all lectures ready</name>
  <files>src/lib/content/module3/index.ts, src/lib/data/modules.ts</files>
  <action>
Update module3/index.ts to export final lectures:

```typescript
// Lecture 9: Sentiment Analysis Application
export { lectureContent as sentimentAnalysisContent } from './sentiment-analysis';

// Lecture 10: Python Demo: Sentiment Pipeline
export { lectureContent as pythonSentimentDemoContent } from './python-sentiment-demo';
```

Update modules.ts to set lectures 9-10 status: 'ready'.

Verify all 10 Module 3 lectures are marked 'ready'.
  </action>
  <verify>All 10 lectures status: 'ready' in module data</verify>
  <done>Module 3 fully complete with all lectures ready</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete Module 3 with Lectures 9-10, glossary, and exam questions</what-built>
  <how-to-verify>
    1. Run: npm run dev
    2. Visit: http://localhost:5174/module/content-analysis
    3. Verify all 10 lectures show "Ready" badge
    4. Click Lecture 9 "Sentiment Analysis Application":
       - Dictionary vs BERT comparison animates
       - Bidirectional context explanation works
       - Pretraining tasks diagram renders
    5. Click Lecture 10 "Python Demo: Sentiment Pipeline":
       - Text preprocessing pipeline animates (5 steps)
       - spaCy processing diagram shows token/lemma/POS
       - Batch processing visualization renders
    6. Navigate to Exam Prep (/exam-prep):
       - Select Module 3 filter
       - Verify questions load (should see ~50 questions)
       - Test a few questions for correct answers
       - Check difficulty distribution looks correct
    7. Navigate to Glossary (/glossary):
       - Filter by Module 3 (or search module 3 terms)
       - Verify NLP concepts appear (BERT, tokenization, etc.)
       - Check definitions are accurate
       - Verify new 'NLP & Text' category appears
    8. Test full navigation through all 10 Module 3 lectures
    9. Verify module stats updated (should show 10 ready lectures)
  </how-to-verify>
  <resume-signal>Type "approved" to complete Module 3, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds
- [ ] All 10 Module 3 lectures accessible and marked ready
- [ ] Sentiment analysis visualizations render
- [ ] Python demo shows NLP pipeline
- [ ] ~40 glossary terms added for Module 3
- [ ] 50 exam questions added for Module 3
- [ ] Exam prep filters work for Module 3
- [ ] Glossary filters work for Module 3
- [ ] New 'nlp' category appears in glossary
</verification>

<success_criteria>
- Lecture 9 covers sentiment analysis with BERT comparison
- Lecture 10 covers Python NLP pipeline implementation
- Module 3 glossary complete (~40 terms)
- Module 3 exam prep complete (50 questions)
- All 10 lectures navigable and functional
- User approved final Module 3 quality
</success_criteria>

<output>
After completion, create `.planning/phases/12-module-3-content/12-04-SUMMARY.md`
</output>
